{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8095fc2b",
   "metadata": {},
   "source": [
    "# Predicting House Prices - Sample Exercise (from Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abcfbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the file train.csv contains house data from iowa\n",
    "iowa_file_path = \"C:\\\\Users\\\\buchh\\\\OneDrive\\\\Desktop\\\\Untitled Folder\\\\resource\\\\train.csv\"\n",
    "home_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34bab1",
   "metadata": {},
   "source": [
    "## Task: Predict housing price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eef986",
   "metadata": {},
   "source": [
    "### Basic Pandas function to review the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(home_data.head())\n",
    "#home_data.describe()\n",
    "#home_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b635a3e",
   "metadata": {},
   "source": [
    "### Finding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "922cc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Id', 'LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath',\n",
    "                 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "\n",
    "# above features are some of the attributes that will be considered\n",
    "# in determining the price\n",
    "y = home_data.SalePrice\n",
    "X = home_data[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198e257",
   "metadata": {},
   "source": [
    "### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0256633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55206fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208500. 181500. 223500. ... 266500. 142125. 147500.]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree is a heuristic model and the model results that\n",
    "# one get today might not be the same as if the model is run some\n",
    "# time in future (it being an NP-complete problem is the reason why)\n",
    "# Thus, in order for it to yeild same results, the value of random_state\n",
    "# is set. The value itself is not important as long as it is there\n",
    "\n",
    "# in this example the model is trained and tested on the same data, which\n",
    "# is not a good idea. The next section is about choosing training and testing data\n",
    "iowa_model = DecisionTreeRegressor(random_state = 1)\n",
    "iowa_model.fit(X, y)\n",
    "\n",
    "predictions = iowa_model.predict(X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6f28c",
   "metadata": {},
   "source": [
    "### Avoiding underfitting and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10936727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tree with 2 layers will have 2^2 (2^depth_of_tree) number of leaves\n",
    "# and the more leaves there are the more complicated the model becomes (overfitting)\n",
    "# and if it is too less then underfitting can happen. In order to avoid that,\n",
    "# the number of leaves are chosen by an algorithm, by passing in some numbers and\n",
    "# picking the one with least mae\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8528579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t Mean Absolute Error:  35044\n",
      "Max leaf nodes: 25  \t Mean Absolute Error:  29016\n",
      "Max leaf nodes: 50  \t Mean Absolute Error:  27405\n",
      "Max leaf nodes: 100  \t Mean Absolute Error:  27282\n",
      "Max leaf nodes: 250  \t Mean Absolute Error:  27893\n",
      "Max leaf nodes: 500  \t Mean Absolute Error:  29454\n"
     ]
    }
   ],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "for max_leaf_nodes in candidate_max_leaf_nodes:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6aca4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the above output, the best number of nodes is the one with least MAE\n",
    "number_of_nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "718e3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important that the train and test data is separated\n",
    "# There is not a single correct answer on how to do it, which is why\n",
    "# train_test_split module is used. The split is based on a random number generator. \n",
    "# Supplying a numeric value to the random_state argument guarantees\n",
    "# we get the same split every time we run this script.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ed5070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 231500 \t Predicted: 181225\n",
      "Actual: 179500 \t Predicted: 173500\n",
      "Actual: 122000 \t Predicted: 122142\n",
      "Actual: 84500 \t Predicted: 77155\n",
      "Actual: 142000 \t Predicted: 148515\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes=number_of_nodes, random_state = 1)\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "predictions = iowa_model.predict(val_X)\n",
    "\n",
    "for ind, prices in enumerate(val_X[:5]['Id']):\n",
    "    print(\"Actual: %d \\t Predicted: %d\" % (home_data.loc[home_data['Id'] == prices]['SalePrice'], predictions[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689133a",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6c071ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error = actual - predicted\n",
    "# In MAE, the absolute values of erros is taken, which converts the\n",
    "# error in a positive number, of which, the avg is taken\n",
    "# It basically means, \"On average, our predictions are off by X\"\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32145fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27967.997329237485\n"
     ]
    }
   ],
   "source": [
    "val_mae = mean_absolute_error(val_y, predictions)\n",
    "print(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8dcde",
   "metadata": {},
   "source": [
    "## Using RandomForest algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random forest uses many trees, and it makes a prediction by averaging \n",
    "# the predictions of each component tree. It generally has much better predictive \n",
    "# accuracy than a single decision tree and it works well with default parameters. \n",
    "# There are many more models with even better performance, \n",
    "# but many of those are sensitive to getting the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "348f9f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Random Forest Model: 22216.7021369863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model. Set random_state to 1\n",
    "rf_model = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "# fit your model\n",
    "rf_model.fit(train_X, train_y)\n",
    "rf_val_pred = rf_model.predict(val_X)\n",
    "\n",
    "# Calculate the mean absolute error of your Random Forest model on the validation data\n",
    "rf_val_mae = mean_absolute_error(val_y, rf_val_pred)\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1327f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
